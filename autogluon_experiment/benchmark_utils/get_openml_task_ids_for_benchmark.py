from __future__ import annotations

from pathlib import Path

import openml
import pandas as pd

benchmark_suite_clf = openml.study.get_suite(271)
benchmark_suite_reg = openml.study.get_suite(269)

task_ids = list(benchmark_suite_clf.tasks)
task_ids += list(benchmark_suite_reg.tasks)

print(f"AutoML Benchmark Task ID: {len(task_ids)} tasks")


# TabRepo Datasets (https://github.com/autogluon/tabrepo/blob/main/tabrepo/contexts/context_2023_08_21.py#L9)
datasets = [
    # Commented out due to excessive size
    # "dionis",  # Cumulative Size: 4.5 TB (244 datasets)
    # "KDDCup99",
    # "Airlines_DepDelay_10M",
    # "Kuzushiji-49",
    # "pokerhand",
    # "sf-police-incidents",
    # "helena",  # Cumulative Size: 1.0 TB (238 datasets)
    # "covertype",
    # "Devnagari-Script",
    # "Higgs",
    # "walking-activity",
    # "spoken-arabic-digit",
    # "GTSRB-HOG01",
    # "GTSRB-HOG02",
    # "GTSRB-HOG03",
    # "GTSRB-HueHist",
    "porto-seguro",  # Cumulative Size: 455 GB (228 datasets)  # 211 datasets (succeeded)
    "airlines",
    "ldpa",
    "albert",
    "tamilnadu-electricity",
    "fars",
    "Buzzinsocialmedia_Twitter",
    "nyc-taxi-green-dec-2016",
    "Fashion-MNIST",
    "Kuzushiji-MNIST",
    "mnist_784",
    # "CIFAR_10",  # Failed
    "volkert",  # 200 datasets (succeeded)
    "Yolanda",
    "letter",
    "kr-vs-k",
    "kropt",
    "MiniBooNE",
    "shuttle",
    "jannis",
    "numerai28_6",
    "Diabetes130US",
    "Run_or_walk_information",
    "APSFailure",
    "kick",
    "Allstate_Claims_Severity",
    "Traffic_violations",
    "black_friday",
    "connect-4",  # Cumulative Size: 107 GB (200 datasets)
    "isolet",
    "adult",
    "okcupid-stem",
    "electricity",
    "bank-marketing",
    # "KDDCup09-Upselling",  # Failed
    # "one-hundred-plants-margin",  # Failed
    # "KDDCup09_appetency",  # Failed
    "jungle_chess_2pcs_raw_endgame_complete",
    "2dplanes",
    "fried",
    "Click_prediction_small",  # 175 datasets (succeeded)
    "nomao",
    "Amazon_employee_access",
    "pendigits",
    "microaggregation2",
    "artificial-characters",
    "robert",
    "houses",
    "Indian_pines",
    "diamonds",
    # "guillermo",  # Failed
    # "riccardo",  # Failed
    # "MagicTelescope",  # Failed
    # "nursery",  # Failed  # Cumulative Size: 50 GB (175 datasets)
    "har",
    "texture",
    "fabert",
    "optdigits",
    "mozilla4",
    "volcanoes-b2",
    "eeg-eye-state",
    "volcanoes-b1",
    "OnlineNewsPopularity",
    "volcanoes-b6",
    "dilbert",
    "volcanoes-b5",
    "GesturePhaseSegmentationProcessed",
    "ailerons",
    "volcanoes-d1",
    "volcanoes-d4",
    "mammography",
    "PhishingWebsites",
    "satimage",
    "jm1",
    "first-order-theorem-proving",
    "kdd_internet_usage",
    "eye_movements",
    "wine-quality-white",
    "delta_elevators",
    "mc1",
    "led24",
    "visualizing_soil",
    "house_16H",
    "SpeedDating",
    "bank32nh",
    "bank8FM",
    "cpu_act",
    "cpu_small",
    "kin8nm",
    "puma32H",
    "puma8NH",
    "collins",
    "house_sales",
    "page-blocks",
    "ringnorm",
    "twonorm",
    "delta_ailerons",
    "wind",
    "wall-robot-navigation",
    "elevators",
    "cardiotocography",
    "philippine",
    "pc2",
    "mfeat-factors",
    # "christine",  # Failed
    "phoneme",
    "sylvine",
    "Satellite",
    "pol",
    "churn",
    "wilt",
    "spambase",
    "segment",
    "waveform-5000",
    # "hypothyroid",  # Failed
    "semeion",
    "hiva_agnostic",
    "ada",
    # "yeast",  # Failed
    "Brazilian_houses",
    "steel-plates-fault",
    "pollen",
    "Bioresponse",  # 100 datasets (succeeded)
    "soybean",
    "Internet-Advertisements",
    "topo_2_1",
    "yprop_4_1",
    "UMIST_Faces_Cropped",
    "madeline",  # Cumulative Size: 8.7 GB (100 datasets)
    "micro-mass",
    "gina",
    "jasmine",
    "splice",
    "dna",
    "wine-quality-red",
    "cnae-9",
    "colleges",
    "madelon",
    "ozone-level-8hr",
    "MiceProtein",
    "volcanoes-a2",
    "volcanoes-a3",
    "Titanic",
    "wine_quality",
    "volcanoes-a4",
    "kc1",
    # "eating",  # Failed
    "car",
    # "QSAR-TID-10980",  # Failed
    # "QSAR-TID-11",  # Failed
    "pbcseq",
    "volcanoes-e1",
    "autoUniv-au6-750",
    # "Santander_transaction_value",  # Failed
    "SAT11-HAND-runtime-regression",
    "GAMETES_Epistasis_2-Way_20atts_0_1H_EDM-1_1",
    "GAMETES_Epistasis_2-Way_1000atts_0_4H_EDM-1_EDM-1_1",
    "GAMETES_Epistasis_2-Way_20atts_0_4H_EDM-1_1",
    "GAMETES_Epistasis_3-Way_20atts_0_2H_EDM-1_1",
    "GAMETES_Heterogeneity_20atts_1600_Het_0_4_0_2_50_EDM-2_001",
    "GAMETES_Heterogeneity_20atts_1600_Het_0_4_0_2_75_EDM-2_001",
    "autoUniv-au7-1100",
    "pc3",
    "Mercedes_Benz_Greener_Manufacturing",
    "OVA_Prostate",
    "OVA_Endometrium",
    "OVA_Kidney",
    "OVA_Lung",
    "OVA_Ovary",
    "pc4",
    # "OVA_Breast",  # Failed
    "OVA_Colon",
    "abalone",
    "LED-display-domain-7digit",
    "analcatdata_dmft",
    "cmc",
    "colleges_usnews",
    # "anneal",  # Failed
    "baseball",
    "hill-valley",
    "space_ga",
    "parity5_plus_5",
    "pc1",
    "eucalyptus",
    "qsar-biodeg",
    "synthetic_control",
    "fri_c0_1000_5",
    "fri_c1_1000_50",
    "fri_c2_1000_25",
    "fri_c3_1000_10",
    "fri_c3_1000_25",
    "autoUniv-au1-1000",
    "credit-g",
    "vehicle",
    "analcatdata_authorship",
    "tokyo1",
    "quake",
    "kdd_el_nino-small",
    "diabetes",  # Cumulative Size: 1.0 GB (30 datasets)
    "blood-transfusion-service-center",
    "us_crime",
    "Australian",
    "autoUniv-au7-700",
    "ilpd",
    "balance-scale",
    "arsenic-female-bladder",
    "climate-model-simulation-crashes",
    "cylinder-bands",
    "meta",
    "house_prices_nominal",
    "kc2",
    "rmftsa_ladata",
    "boston_corrected",
    "fri_c0_500_5",
    "fri_c2_500_50",
    "fri_c3_500_10",
    "fri_c4_500_100",
    "no2",
    "pm10",
    "dresses-sales",
    "fri_c3_500_50",
    "Moneyball",
    "socmob",
    "MIP-2016-regression",
    "sensory",
    "boston",
    "arcene",
    "tecator",
][-200:]  # filter for context used in the paper: D244_F3_C1530_200

# Make sure to download the dataset metadata (task_metadata_289.csv) from: https://github.com/autogluon/tabrepo/blob/main/data/metadata/task_metadata_289.csv
dataset_metadata = pd.read_csv(Path(__file__).parent / "task_metadata_289.csv", index_col=False)
tid_name_map = dataset_metadata[["name", "tid"]].set_index("tid").to_dict()["name"]

task_ids = [task_id for task_id in task_ids if tid_name_map[task_id] in datasets]
print(f"After TabRepo Filter: {len(task_ids)} tasks")
print(task_ids)

expected_results = [
    146818,
    146820,
    168350,
    168757,
    168784,
    168868,
    168909,
    168910,
    168911,
    189922,
    190137,
    190146,
    190392,
    190410,
    190411,
    190412,
    211979,
    211986,
    359953,
    359954,
    359955,
    359956,
    359957,
    359958,
    359959,
    359960,
    359961,
    359962,
    359963,
    359964,
    359966,
    359967,
    359968,
    359969,
    359970,
    359971,
    359972,
    359974,
    359975,
    359977,
    359979,
    359980,
    359981,
    359982,
    359983,
    359985,
    359986,
    359987,
    359990,
    359991,
    359992,
    359993,
    167210,
    233211,
    233212,
    233215,
    317614,
    359930,
    359931,
    359932,
    359933,
    359934,
    359935,
    359936,
    359937,
    359938,
    359939,
    359940,
    359941,
    359942,
    359944,
    359945,
    359946,
    359948,
    359949,
    359950,
    359951,
    359952,
    360945,
]
assert task_ids == expected_results
