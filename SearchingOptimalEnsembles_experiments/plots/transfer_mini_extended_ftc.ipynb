{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance_matrix\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pineda/anaconda3/envs/searching_optimal_ensembles/lib/python3.10/site-packages/dask/dataframe/_pyarrow_compat.py:23: UserWarning: You are using pyarrow version 13.0.0 which is known to be insecure. See https://www.cve.org/CVERecord?id=CVE-2023-47248 for further details. Please upgrade to pyarrow>=14.0.1 or install pyarrow-hotfix to patch your current version.\n",
      "  warnings.warn(\n",
      "/home/pineda/anaconda3/envs/searching_optimal_ensembles/lib/python3.10/site-packages/dask/dataframe/__init__.py:31: FutureWarning: \n",
      "Dask dataframe query planning is disabled because dask-expr is not installed.\n",
      "\n",
      "You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
      "This will raise in a future version.\n",
      "\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from SearchingOptimalEnsembles.metadatasets.ftc.metadataset import FTCMetaDataset\n",
    "from SearchingOptimalEnsembles.metadatasets.ftc.hub import MODELS\n",
    "from SearchingOptimalEnsembles.posthoc.neural_ensembler import NeuralEnsembler\n",
    "from SearchingOptimalEnsembles.posthoc.greedy_ensembler import GreedyEnsembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess(X):\n",
    "    X = pd.DataFrame(X)\n",
    "    ohe = OneHotEncoder(handle_unknown='ignore', categories=[MODELS])\n",
    "    X_model = ohe.fit_transform(X[[\"model\"]].values).todense()\n",
    "    X = np.concatenate([X[[\"lora_r\", \"learning_rate\"]], X_model], axis=-1)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GreedyEnsembler: 0\n",
      "GreedyEnsembler: 1\n",
      "GreedyEnsembler: 2\n",
      "GreedyEnsembler: 3\n",
      "GreedyEnsembler: 4\n",
      "[118, 94, 82, 84] [118  94  82  84]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pineda/SearchingOptimalEnsembles/SearchingOptimalEnsembles/metadatasets/ftc/metadataset.py:197: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:274.)\n",
      "  return self.times[self.dataset_name][torch.LongTensor(ensembles)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imdb tensor([0.0376])\n",
      "GreedyEnsembler: 0\n",
      "GreedyEnsembler: 1\n",
      "GreedyEnsembler: 2\n",
      "GreedyEnsembler: 3\n",
      "GreedyEnsembler: 4\n",
      "[68, 57, 93, 79, 23] [68 57 93 79 23]\n",
      "mteb/tweet_sentiment_extraction tensor([0.1930])\n",
      "GreedyEnsembler: 0\n",
      "GreedyEnsembler: 1\n",
      "GreedyEnsembler: 2\n",
      "GreedyEnsembler: 3\n",
      "GreedyEnsembler: 4\n",
      "[98, 44, 83, 73, 2] [98 44 83 73  2]\n",
      "ag_news tensor([0.0588])\n",
      "GreedyEnsembler: 0\n",
      "GreedyEnsembler: 1\n",
      "GreedyEnsembler: 2\n",
      "GreedyEnsembler: 3\n",
      "GreedyEnsembler: 4\n",
      "[18, 23, 12, 10] [18 23 12 10]\n",
      "dbpedia_14 tensor([0.0080])\n",
      "GreedyEnsembler: 0\n",
      "GreedyEnsembler: 1\n",
      "GreedyEnsembler: 2\n",
      "GreedyEnsembler: 3\n",
      "GreedyEnsembler: 4\n",
      "[118, 123, 105, 84] [118 123 105  84]\n",
      "stanfordnlp/sst2 tensor([0.0382])\n",
      "GreedyEnsembler: 0\n",
      "GreedyEnsembler: 1\n",
      "GreedyEnsembler: 2\n",
      "GreedyEnsembler: 3\n",
      "GreedyEnsembler: 4\n",
      "[13, 3, 8, 23, 22] [13  3  8 23 22]\n"
     ]
    }
   ],
   "source": [
    "dataset_name = 'imdb'\n",
    "data_verion=\"mini\"\n",
    "\n",
    "metadataset_mini = FTCMetaDataset( metric_name=\"error\",\n",
    "                                data_version=\"mini\")\n",
    "metadataset_extended = FTCMetaDataset( metric_name=\"error\",\n",
    "                        data_version=\"extended\")  \n",
    "\n",
    "results = []\n",
    "for dataset_name in metadataset_extended.get_dataset_names():\n",
    "    metadataset_mini.set_state(dataset_name=dataset_name, split=\"valid\")\n",
    "    metadataset_extended.set_state(dataset_name=dataset_name, split=\"valid\")\n",
    "\n",
    "    hp_mini = metadataset_mini.row_hp_candidates[dataset_name]\n",
    "    hp_extended = metadataset_extended.row_hp_candidates[dataset_name]\n",
    "\n",
    "    hp_mini = preprocess(hp_mini)\n",
    "    hp_extended = preprocess(hp_extended)\n",
    "\n",
    "        \n",
    "    a = distance_matrix(hp_mini, hp_extended)\n",
    "    from_mini_to_extended = a.argmin(-1)[a.min(-1)==0]\n",
    "    from_extended_to_mini = a.argmin(0)[a.min(0)==0]\n",
    "\n",
    "    ge = GreedyEnsembler(metadataset=metadataset_mini,\n",
    "                        max_num_pipelines=5)\n",
    "    X_obs_extended = np.arange(len(hp_extended))\n",
    "    X_obs_mini = from_extended_to_mini[X_obs_extended]\n",
    "    a = ge.sample(X_obs_mini)\n",
    "    best_ensemble_mini = ge.best_ensemble\n",
    "\n",
    "    best_ensemble_extended = from_mini_to_extended[best_ensemble_mini]\n",
    "    print(best_ensemble_mini, best_ensemble_extended)\n",
    "    metadataset_mini.set_state(dataset_name=dataset_name,\n",
    "                            split=\"test\")\n",
    "    metadataset_extended.set_state(dataset_name=dataset_name,\n",
    "                            split=\"test\")\n",
    "    output = metadataset_extended.evaluate_ensembles([best_ensemble_extended])\n",
    "    print(dataset_name, output[1])\n",
    "    results.append([dataset_name, output[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m results\n",
      "\u001b[0;31mNameError\u001b[0m: name 'results' is not defined"
     ]
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GreedyEnsembler: 0\n",
      "GreedyEnsembler: 1\n",
      "GreedyEnsembler: 2\n",
      "GreedyEnsembler: 3\n",
      "GreedyEnsembler: 4\n",
      "[124, 98, 118, 84, 87] [124  98 118  84  87]\n",
      "imdb tensor([0.1187])\n",
      "GreedyEnsembler: 0\n",
      "GreedyEnsembler: 1\n",
      "GreedyEnsembler: 2\n",
      "GreedyEnsembler: 3\n",
      "GreedyEnsembler: 4\n",
      "[54, 57, 73, 81, 28] [54 57 73 81 28]\n",
      "mteb/tweet_sentiment_extraction tensor([0.5477])\n",
      "GreedyEnsembler: 0\n",
      "GreedyEnsembler: 1\n",
      "GreedyEnsembler: 2\n",
      "GreedyEnsembler: 3\n",
      "GreedyEnsembler: 4\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 99 is out of bounds for axis 0 with size 99",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 30\u001b[0m\n\u001b[1;32m     27\u001b[0m ge\u001b[39m.\u001b[39msample(X_obs_mini)\n\u001b[1;32m     28\u001b[0m best_ensemble_mini \u001b[39m=\u001b[39m ge\u001b[39m.\u001b[39mbest_ensemble\n\u001b[0;32m---> 30\u001b[0m best_ensemble_extended \u001b[39m=\u001b[39m from_mini_to_extended[best_ensemble_mini]\n\u001b[1;32m     31\u001b[0m \u001b[39mprint\u001b[39m(best_ensemble_mini, best_ensemble_extended)\n\u001b[1;32m     32\u001b[0m metadataset_mini\u001b[39m.\u001b[39mset_state(dataset_name\u001b[39m=\u001b[39mdataset_name,\n\u001b[1;32m     33\u001b[0m                         split\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mIndexError\u001b[0m: index 99 is out of bounds for axis 0 with size 99"
     ]
    }
   ],
   "source": [
    "metric_name = \"nll\"\n",
    "metadataset_mini = FTCMetaDataset( metric_name=metric_name,\n",
    "                                data_version=\"mini\")\n",
    "metadataset_extended = FTCMetaDataset( metric_name=metric_name,\n",
    "                        data_version=\"extended\")  \n",
    "\n",
    "results = []\n",
    "for dataset_name in metadataset_extended.get_dataset_names():\n",
    "    metadataset_mini.set_state(dataset_name=dataset_name, split=\"valid\")\n",
    "    metadataset_extended.set_state(dataset_name=dataset_name, split=\"valid\")\n",
    "\n",
    "    hp_mini = metadataset_mini.row_hp_candidates[dataset_name]\n",
    "    hp_extended = metadataset_extended.row_hp_candidates[dataset_name]\n",
    "\n",
    "    hp_mini = preprocess(hp_mini)\n",
    "    hp_extended = preprocess(hp_extended)\n",
    "\n",
    "        \n",
    "    a = distance_matrix(hp_mini, hp_extended)\n",
    "    from_mini_to_extended = a.argmin(-1)[a.min(-1)==0]\n",
    "    from_extended_to_mini = a.argmin(0)[a.min(0)==0][:len(hp_extended)]\n",
    "\n",
    "    ge = GreedyEnsembler(metadataset=metadataset_mini,\n",
    "                        max_num_pipelines=5)\n",
    "    X_obs_extended = np.arange(len(hp_extended))\n",
    "    X_obs_mini = from_extended_to_mini[X_obs_extended]\n",
    "    X_obs_mini = [x for x in X_obs_mini if x in X_obs_extended.tolist()]\n",
    "    ge.sample(X_obs_mini)\n",
    "    best_ensemble_mini = ge.best_ensemble\n",
    "\n",
    "    best_ensemble_extended = from_mini_to_extended[best_ensemble_mini]\n",
    "    print(best_ensemble_mini, best_ensemble_extended)\n",
    "    metadataset_mini.set_state(dataset_name=dataset_name,\n",
    "                            split=\"test\")\n",
    "    metadataset_extended.set_state(dataset_name=dataset_name,\n",
    "                            split=\"test\")\n",
    "    output = metadataset_extended.evaluate_ensembles([best_ensemble_extended])\n",
    "    print(dataset_name, output[1])\n",
    "    results.append([dataset_name, output[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
       "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
       "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 62, 63, 64, 65, 66, 67, 68,\n",
       "       69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85,\n",
       "       86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from_extended_to_mini[X_obs_extended]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
       "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
       "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 62, 63, 64, 65, 66, 67, 68,\n",
       "       69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85,\n",
       "       86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from_extended_to_mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99, 7)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hp_extended.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([99, 98, 38, 16, 93], tensor(0.2232))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120, 7)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hp_mini.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['imdb',\n",
       " 'mteb/tweet_sentiment_extraction',\n",
       " 'ag_news',\n",
       " 'dbpedia_14',\n",
       " 'stanfordnlp/sst2',\n",
       " 'SetFit/mnli']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadataset_extended.get_dataset_names(\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadataset_extended.evaluate_ensembles([best_ensemble_extended])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "searching_optimal_ensembles",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14 (main, May  6 2024, 19:42:50) [GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "501a77d84e0083a1edac1feed26a26e2814e751f8bdd1662ff19a2dbb6f51c09"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
